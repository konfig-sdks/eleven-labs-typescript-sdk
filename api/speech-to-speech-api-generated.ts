/* tslint:disable */
/* eslint-disable */
/*
ElevenLabs API Documentation

This is the documentation for the ElevenLabs API. You can use this API to use our service programmatically, this is done by using your xi-api-key. <br/> You can view your xi-api-key using the 'Profile' tab on https://elevenlabs.io. Our API is experimental so all endpoints are subject to change.

The version of the OpenAPI document: 1.0


NOTE: This file is auto generated by Konfig (https://konfigthis.com).
*/

import globalAxios, { AxiosPromise, AxiosInstance, AxiosRequestConfig } from 'axios';
import { Configuration } from '../configuration';
// Some imports not used depending on template conditions
// @ts-ignore
import { DUMMY_BASE_URL, assertParamExists, setApiKeyToObject, setBasicAuthToObject, setBearerAuthToObject, setSearchParams, serializeDataIfNeeded, toPathString, createRequestFunction, isBrowser } from '../common';
import { fromBuffer } from "file-type/browser"
const FormData = require("form-data")
// @ts-ignore
import { BASE_PATH, COLLECTION_FORMATS, RequestArgs, BaseAPI, RequiredError } from '../base';
// @ts-ignore
import { BodySpeechToSpeechStreamingV1SpeechToSpeechVoiceIdStreamPost } from '../models';
// @ts-ignore
import { BodySpeechToSpeechV1SpeechToSpeechVoiceIdPost } from '../models';
// @ts-ignore
import { HTTPValidationError } from '../models';
import { paginate } from "../pagination/paginate";
import type * as buffer from "buffer"
import { requestBeforeHook } from '../requestBeforeHook';
/**
 * SpeechToSpeechApi - axios parameter creator
 * @export
 */
export const SpeechToSpeechApiAxiosParamCreator = function (configuration?: Configuration) {
    return {
        /**
         * Create speech by combining the content and emotion of the uploaded audio with a voice of your choice.
         * @summary Speech To Speech
         * @param {string} voiceId Voice ID to be used, you can use https://api.elevenlabs.io/v1/voices to list all the available voices.
         * @param {Uint8Array | File | buffer.File} audio The audio file which holds the content and emotion that will control the generated speech.
         * @param {BodySpeechToSpeechV1SpeechToSpeechVoiceIdPost} bodySpeechToSpeechV1SpeechToSpeechVoiceIdPost 
         * @param {number} [optimizeStreamingLatency] You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values: 0 - default mode (no latency optimizations) 1 - normal latency optimizations (about 50% of possible latency improvement of option 3) 2 - strong latency optimizations (about 75% of possible latency improvement of option 3) 3 - max latency optimizations 4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).  Defaults to 0. 
         * @param {string} [xiApiKey] Your API key. This is required by most endpoints to access our API programatically. You can view your xi-api-key using the \&#39;Profile\&#39; tab on the website.
         * @param {string} [modelId] Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for speech to speech, you can check this using the can_do_voice_conversion property.
         * @param {string} [voiceSettings] Voice settings overriding stored setttings for the given voice. They are applied only on the given request. Needs to be send as a JSON encoded string.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        createWithVoice: async (voiceId: string, audio: Uint8Array | File | buffer.File, bodySpeechToSpeechV1SpeechToSpeechVoiceIdPost: BodySpeechToSpeechV1SpeechToSpeechVoiceIdPost, optimizeStreamingLatency?: number, xiApiKey?: string, modelId?: string, voiceSettings?: string, options: AxiosRequestConfig = {}): Promise<RequestArgs> => {
            // verify required parameter 'voiceId' is not null or undefined
            assertParamExists('createWithVoice', 'voiceId', voiceId)
            // verify required parameter 'audio' is not null or undefined
            assertParamExists('createWithVoice', 'audio', audio)
            // verify required parameter 'bodySpeechToSpeechV1SpeechToSpeechVoiceIdPost' is not null or undefined
            assertParamExists('createWithVoice', 'bodySpeechToSpeechV1SpeechToSpeechVoiceIdPost', bodySpeechToSpeechV1SpeechToSpeechVoiceIdPost)
            const localVarPath = `/v1/speech-to-speech/{voice_id}`
                .replace(`{${"voice_id"}}`, encodeURIComponent(String(voiceId !== undefined ? voiceId : `-voice_id-`)));
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions: AxiosRequestConfig = { method: 'POST', ...baseOptions, ...options};
            const localVarHeaderParameter = configuration && !isBrowser() ? { "User-Agent": configuration.userAgent } : {} as any;
            const localVarQueryParameter = {} as any;
            const localVarFormParams = new ((configuration && configuration.formDataCtor) || FormData)();
            const addFormParam = async (name: string, data: any, isBinary: boolean, isPrimitiveType: boolean) => {
                if (isBinary) {
                    if (data instanceof Uint8Array) {
                        // Handle Buffer data
                        const filetype = await fromBuffer(data)
                        const filename = filetype === undefined ? name : `${name}.${filetype.ext}`
                        localVarFormParams.append(name, data as any, filename);
                    } else if ("name" in data) {
                        // File instances in browsers and Node.js have the
                        // "name" property "Duck typing" files to handle browser
                        // File class or Node.js File class
                        // Web: https://developer.mozilla.org/en-US/docs/Web/API/File
                        // Node.js: https://nodejs.org/api/buffer.html#new-bufferfilesources-filename-options
                        if (isBrowser()) {
                            // FormData in browser can accept File/Blob directly
                            localVarFormParams.append(name, data, data.name);
                        } else {
                            // FormData in Node.js can only accept raw Buffer so convert before passing
                            const bytes = await data.arrayBuffer()
                            const buffer = Buffer.from(bytes)
                            localVarFormParams.append(name, buffer, data.name);
                        }
                    }
                } else {
                    if (isPrimitiveType) {
                        /**
                         * FormData can only accept string or Blob so we need to convert
                         * non-string primitives to string. We also need to convert
                         */
                        if (typeof data === "object") {
                          localVarFormParams.append(name, JSON.stringify(data));
                        } else {
                          localVarFormParams.append(name, data);
                        }
                    } else {
                        if (isBrowser()) {
                            localVarFormParams.append(name, new Blob([JSON.stringify(data)], { type: "application/json" }))
                        } else {
                            localVarFormParams.append(name, JSON.stringify(data), { type: "application/json", filename: "data.json" });
                        }
                    }
                }
            }
            if (!isBrowser()) Object.assign(localVarHeaderParameter, localVarFormParams.getHeaders());

            if (optimizeStreamingLatency !== undefined) {
                localVarQueryParameter['optimize_streaming_latency'] = optimizeStreamingLatency;
            }

            if (xiApiKey != null) {
                localVarHeaderParameter['xi-api-key'] = String(xiApiKey);
            }


            if (audio !== undefined) {
                await addFormParam('audio', audio, true, true)
            }
    
            if (modelId !== undefined) {
                await addFormParam('model_id', modelId, false, true)
            }
    
            if (voiceSettings !== undefined) {
                await addFormParam('voice_settings', voiceSettings, false, true)
            }
    
    
    
            localVarHeaderParameter['Content-Type'] = 'multipart/form-data';


            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};
            localVarRequestOptions.data = localVarFormParams;
            requestBeforeHook({
                requestBody: bodySpeechToSpeechV1SpeechToSpeechVoiceIdPost,
                queryParameters: localVarQueryParameter,
                requestConfig: localVarRequestOptions,
                path: localVarPath,
                configuration,
                pathTemplate: '/v1/speech-to-speech/{voice_id}',
                httpMethod: 'POST'
            });

            setSearchParams(localVarUrlObj, localVarQueryParameter);
            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
        /**
         * Create speech by combining the content and emotion of the uploaded audio with a voice of your choice and returns an audio stream.
         * @summary Speech To Speech Streaming
         * @param {string} voiceId Voice ID to be used, you can use https://api.elevenlabs.io/v1/voices to list all the available voices.
         * @param {Uint8Array | File | buffer.File} audio The audio file which holds the content and emotion that will control the generated speech.
         * @param {BodySpeechToSpeechStreamingV1SpeechToSpeechVoiceIdStreamPost} bodySpeechToSpeechStreamingV1SpeechToSpeechVoiceIdStreamPost 
         * @param {number} [optimizeStreamingLatency] You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values: 0 - default mode (no latency optimizations) 1 - normal latency optimizations (about 50% of possible latency improvement of option 3) 2 - strong latency optimizations (about 75% of possible latency improvement of option 3) 3 - max latency optimizations 4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).  Defaults to 0. 
         * @param {string} [xiApiKey] Your API key. This is required by most endpoints to access our API programatically. You can view your xi-api-key using the \&#39;Profile\&#39; tab on the website.
         * @param {string} [modelId] Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for speech to speech, you can check this using the can_do_voice_conversion property.
         * @param {string} [voiceSettings] Voice settings overriding stored setttings for the given voice. They are applied only on the given request. Needs to be send as a JSON encoded string.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        createWithVoice_1: async (voiceId: string, audio: Uint8Array | File | buffer.File, bodySpeechToSpeechStreamingV1SpeechToSpeechVoiceIdStreamPost: BodySpeechToSpeechStreamingV1SpeechToSpeechVoiceIdStreamPost, optimizeStreamingLatency?: number, xiApiKey?: string, modelId?: string, voiceSettings?: string, options: AxiosRequestConfig = {}): Promise<RequestArgs> => {
            // verify required parameter 'voiceId' is not null or undefined
            assertParamExists('createWithVoice_1', 'voiceId', voiceId)
            // verify required parameter 'audio' is not null or undefined
            assertParamExists('createWithVoice_1', 'audio', audio)
            // verify required parameter 'bodySpeechToSpeechStreamingV1SpeechToSpeechVoiceIdStreamPost' is not null or undefined
            assertParamExists('createWithVoice_1', 'bodySpeechToSpeechStreamingV1SpeechToSpeechVoiceIdStreamPost', bodySpeechToSpeechStreamingV1SpeechToSpeechVoiceIdStreamPost)
            const localVarPath = `/v1/speech-to-speech/{voice_id}/stream`
                .replace(`{${"voice_id"}}`, encodeURIComponent(String(voiceId !== undefined ? voiceId : `-voice_id-`)));
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions: AxiosRequestConfig = { method: 'POST', ...baseOptions, ...options};
            const localVarHeaderParameter = configuration && !isBrowser() ? { "User-Agent": configuration.userAgent } : {} as any;
            const localVarQueryParameter = {} as any;
            const localVarFormParams = new ((configuration && configuration.formDataCtor) || FormData)();
            const addFormParam = async (name: string, data: any, isBinary: boolean, isPrimitiveType: boolean) => {
                if (isBinary) {
                    if (data instanceof Uint8Array) {
                        // Handle Buffer data
                        const filetype = await fromBuffer(data)
                        const filename = filetype === undefined ? name : `${name}.${filetype.ext}`
                        localVarFormParams.append(name, data as any, filename);
                    } else if ("name" in data) {
                        // File instances in browsers and Node.js have the
                        // "name" property "Duck typing" files to handle browser
                        // File class or Node.js File class
                        // Web: https://developer.mozilla.org/en-US/docs/Web/API/File
                        // Node.js: https://nodejs.org/api/buffer.html#new-bufferfilesources-filename-options
                        if (isBrowser()) {
                            // FormData in browser can accept File/Blob directly
                            localVarFormParams.append(name, data, data.name);
                        } else {
                            // FormData in Node.js can only accept raw Buffer so convert before passing
                            const bytes = await data.arrayBuffer()
                            const buffer = Buffer.from(bytes)
                            localVarFormParams.append(name, buffer, data.name);
                        }
                    }
                } else {
                    if (isPrimitiveType) {
                        /**
                         * FormData can only accept string or Blob so we need to convert
                         * non-string primitives to string. We also need to convert
                         */
                        if (typeof data === "object") {
                          localVarFormParams.append(name, JSON.stringify(data));
                        } else {
                          localVarFormParams.append(name, data);
                        }
                    } else {
                        if (isBrowser()) {
                            localVarFormParams.append(name, new Blob([JSON.stringify(data)], { type: "application/json" }))
                        } else {
                            localVarFormParams.append(name, JSON.stringify(data), { type: "application/json", filename: "data.json" });
                        }
                    }
                }
            }
            if (!isBrowser()) Object.assign(localVarHeaderParameter, localVarFormParams.getHeaders());

            if (optimizeStreamingLatency !== undefined) {
                localVarQueryParameter['optimize_streaming_latency'] = optimizeStreamingLatency;
            }

            if (xiApiKey != null) {
                localVarHeaderParameter['xi-api-key'] = String(xiApiKey);
            }


            if (audio !== undefined) {
                await addFormParam('audio', audio, true, true)
            }
    
            if (modelId !== undefined) {
                await addFormParam('model_id', modelId, false, true)
            }
    
            if (voiceSettings !== undefined) {
                await addFormParam('voice_settings', voiceSettings, false, true)
            }
    
    
    
            localVarHeaderParameter['Content-Type'] = 'multipart/form-data';


            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};
            localVarRequestOptions.data = localVarFormParams;
            requestBeforeHook({
                requestBody: bodySpeechToSpeechStreamingV1SpeechToSpeechVoiceIdStreamPost,
                queryParameters: localVarQueryParameter,
                requestConfig: localVarRequestOptions,
                path: localVarPath,
                configuration,
                pathTemplate: '/v1/speech-to-speech/{voice_id}/stream',
                httpMethod: 'POST'
            });

            setSearchParams(localVarUrlObj, localVarQueryParameter);
            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
    }
};

/**
 * SpeechToSpeechApi - functional programming interface
 * @export
 */
export const SpeechToSpeechApiFp = function(configuration?: Configuration) {
    const localVarAxiosParamCreator = SpeechToSpeechApiAxiosParamCreator(configuration)
    return {
        /**
         * Create speech by combining the content and emotion of the uploaded audio with a voice of your choice.
         * @summary Speech To Speech
         * @param {SpeechToSpeechApiCreateWithVoiceRequest} requestParameters Request parameters.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async createWithVoice(requestParameters: SpeechToSpeechApiCreateWithVoiceRequest, options?: AxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<void>> {
            const bodySpeechToSpeechV1SpeechToSpeechVoiceIdPost: BodySpeechToSpeechV1SpeechToSpeechVoiceIdPost = {
                audio: requestParameters.audio,
                model_id: requestParameters.model_id,
                voice_settings: requestParameters.voice_settings
            };
            const localVarAxiosArgs = await localVarAxiosParamCreator.createWithVoice(requestParameters.voiceId, requestParameters.audio, bodySpeechToSpeechV1SpeechToSpeechVoiceIdPost, requestParameters.optimizeStreamingLatency, requestParameters.xiApiKey, requestParameters.modelId, requestParameters.voiceSettings, options);
            return createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration);
        },
        /**
         * Create speech by combining the content and emotion of the uploaded audio with a voice of your choice and returns an audio stream.
         * @summary Speech To Speech Streaming
         * @param {SpeechToSpeechApiCreateWithVoice0Request} requestParameters Request parameters.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async createWithVoice_1(requestParameters: SpeechToSpeechApiCreateWithVoice0Request, options?: AxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<void>> {
            const bodySpeechToSpeechStreamingV1SpeechToSpeechVoiceIdStreamPost: BodySpeechToSpeechStreamingV1SpeechToSpeechVoiceIdStreamPost = {
                audio: requestParameters.audio,
                model_id: requestParameters.model_id,
                voice_settings: requestParameters.voice_settings
            };
            const localVarAxiosArgs = await localVarAxiosParamCreator.createWithVoice_1(requestParameters.voiceId, requestParameters.audio, bodySpeechToSpeechStreamingV1SpeechToSpeechVoiceIdStreamPost, requestParameters.optimizeStreamingLatency, requestParameters.xiApiKey, requestParameters.modelId, requestParameters.voiceSettings, options);
            return createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration);
        },
    }
};

/**
 * SpeechToSpeechApi - factory interface
 * @export
 */
export const SpeechToSpeechApiFactory = function (configuration?: Configuration, basePath?: string, axios?: AxiosInstance) {
    const localVarFp = SpeechToSpeechApiFp(configuration)
    return {
        /**
         * Create speech by combining the content and emotion of the uploaded audio with a voice of your choice.
         * @summary Speech To Speech
         * @param {SpeechToSpeechApiCreateWithVoiceRequest} requestParameters Request parameters.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        createWithVoice(requestParameters: SpeechToSpeechApiCreateWithVoiceRequest, options?: AxiosRequestConfig): AxiosPromise<void> {
            return localVarFp.createWithVoice(requestParameters, options).then((request) => request(axios, basePath));
        },
        /**
         * Create speech by combining the content and emotion of the uploaded audio with a voice of your choice and returns an audio stream.
         * @summary Speech To Speech Streaming
         * @param {SpeechToSpeechApiCreateWithVoice0Request} requestParameters Request parameters.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        createWithVoice_1(requestParameters: SpeechToSpeechApiCreateWithVoice0Request, options?: AxiosRequestConfig): AxiosPromise<void> {
            return localVarFp.createWithVoice_1(requestParameters, options).then((request) => request(axios, basePath));
        },
    };
};

/**
 * Request parameters for createWithVoice operation in SpeechToSpeechApi.
 * @export
 * @interface SpeechToSpeechApiCreateWithVoiceRequest
 */
export type SpeechToSpeechApiCreateWithVoiceRequest = {
    
    /**
    * Voice ID to be used, you can use https://api.elevenlabs.io/v1/voices to list all the available voices.
    * @type {string}
    * @memberof SpeechToSpeechApiCreateWithVoice
    */
    readonly voiceId: string
    
    /**
    * The audio file which holds the content and emotion that will control the generated speech.
    * @type {Uint8Array | File | buffer.File}
    * @memberof SpeechToSpeechApiCreateWithVoice
    */
    readonly audio: Uint8Array | File | buffer.File
    
    /**
    * You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values: 0 - default mode (no latency optimizations) 1 - normal latency optimizations (about 50% of possible latency improvement of option 3) 2 - strong latency optimizations (about 75% of possible latency improvement of option 3) 3 - max latency optimizations 4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).  Defaults to 0. 
    * @type {number}
    * @memberof SpeechToSpeechApiCreateWithVoice
    */
    readonly optimizeStreamingLatency?: number
    
    /**
    * Your API key. This is required by most endpoints to access our API programatically. You can view your xi-api-key using the \'Profile\' tab on the website.
    * @type {string}
    * @memberof SpeechToSpeechApiCreateWithVoice
    */
    readonly xiApiKey?: string
    
    /**
    * Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for speech to speech, you can check this using the can_do_voice_conversion property.
    * @type {string}
    * @memberof SpeechToSpeechApiCreateWithVoice
    */
    readonly modelId?: string
    
    /**
    * Voice settings overriding stored setttings for the given voice. They are applied only on the given request. Needs to be send as a JSON encoded string.
    * @type {string}
    * @memberof SpeechToSpeechApiCreateWithVoice
    */
    readonly voiceSettings?: string
    
} & BodySpeechToSpeechV1SpeechToSpeechVoiceIdPost

/**
 * Request parameters for createWithVoice_1 operation in SpeechToSpeechApi.
 * @export
 * @interface SpeechToSpeechApiCreateWithVoice0Request
 */
export type SpeechToSpeechApiCreateWithVoice0Request = {
    
    /**
    * Voice ID to be used, you can use https://api.elevenlabs.io/v1/voices to list all the available voices.
    * @type {string}
    * @memberof SpeechToSpeechApiCreateWithVoice0
    */
    readonly voiceId: string
    
    /**
    * The audio file which holds the content and emotion that will control the generated speech.
    * @type {Uint8Array | File | buffer.File}
    * @memberof SpeechToSpeechApiCreateWithVoice0
    */
    readonly audio: Uint8Array | File | buffer.File
    
    /**
    * You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values: 0 - default mode (no latency optimizations) 1 - normal latency optimizations (about 50% of possible latency improvement of option 3) 2 - strong latency optimizations (about 75% of possible latency improvement of option 3) 3 - max latency optimizations 4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).  Defaults to 0. 
    * @type {number}
    * @memberof SpeechToSpeechApiCreateWithVoice0
    */
    readonly optimizeStreamingLatency?: number
    
    /**
    * Your API key. This is required by most endpoints to access our API programatically. You can view your xi-api-key using the \'Profile\' tab on the website.
    * @type {string}
    * @memberof SpeechToSpeechApiCreateWithVoice0
    */
    readonly xiApiKey?: string
    
    /**
    * Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for speech to speech, you can check this using the can_do_voice_conversion property.
    * @type {string}
    * @memberof SpeechToSpeechApiCreateWithVoice0
    */
    readonly modelId?: string
    
    /**
    * Voice settings overriding stored setttings for the given voice. They are applied only on the given request. Needs to be send as a JSON encoded string.
    * @type {string}
    * @memberof SpeechToSpeechApiCreateWithVoice0
    */
    readonly voiceSettings?: string
    
} & BodySpeechToSpeechStreamingV1SpeechToSpeechVoiceIdStreamPost

/**
 * SpeechToSpeechApiGenerated - object-oriented interface
 * @export
 * @class SpeechToSpeechApiGenerated
 * @extends {BaseAPI}
 */
export class SpeechToSpeechApiGenerated extends BaseAPI {
    /**
     * Create speech by combining the content and emotion of the uploaded audio with a voice of your choice.
     * @summary Speech To Speech
     * @param {SpeechToSpeechApiCreateWithVoiceRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof SpeechToSpeechApiGenerated
     */
    public createWithVoice(requestParameters: SpeechToSpeechApiCreateWithVoiceRequest, options?: AxiosRequestConfig) {
        return SpeechToSpeechApiFp(this.configuration).createWithVoice(requestParameters, options).then((request) => request(this.axios, this.basePath));
    }

    /**
     * Create speech by combining the content and emotion of the uploaded audio with a voice of your choice and returns an audio stream.
     * @summary Speech To Speech Streaming
     * @param {SpeechToSpeechApiCreateWithVoice0Request} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof SpeechToSpeechApiGenerated
     */
    public createWithVoice_1(requestParameters: SpeechToSpeechApiCreateWithVoice0Request, options?: AxiosRequestConfig) {
        return SpeechToSpeechApiFp(this.configuration).createWithVoice_1(requestParameters, options).then((request) => request(this.axios, this.basePath));
    }
}
